import os.path
from tqdm import tqdm
import multiprocessing

import sys
import os

import collections
import numpy as np

from pprint import pprint

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

from core.droidfeature import feature_gen
from tools import utils
from config import logging, ErrorHandler

logger = logging.getLogger('core.droidfeature.feature_extraction')
logger.addHandler(ErrorHandler)


class Apk2features(object):
    """Get features from an APK"""

    def __init__(self,
                 naive_data_save_dir, # ç”¨äºä¿å­˜ä¸­é—´æ•°æ®çš„ç›®å½•
                 intermediate_save_dir, # ç”¨äºä¿å­˜ç‰¹å¾ pickle æ–‡ä»¶çš„ç›®å½•
                 number_of_smali_files=1000000, # å¤„ç†çš„ smali æ–‡ä»¶çš„æœ€å¤§æ•°é‡ï¼Œé»˜è®¤ä¸º 1000000ã€‚
                 max_vocab_size=10000, # è¯æ±‡è¡¨çš„æœ€å¤§å¤§å°ï¼Œé»˜è®¤ä¸º 10000
                 file_ext='.feat', # æ–‡ä»¶æ‰©å±•åï¼Œé»˜è®¤ä¸º '.feat'
                 update=False,  # è¡¨ç¤ºæ˜¯å¦é‡æ–°è®¡ç®—åŸå§‹ç‰¹å¾ï¼Œé»˜è®¤ä¸º False
                 proc_number=2, # è¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸º 2
                 **kwargs  
                 ):
        """
        initialization
        :param naive_data_save_dir: a directory for saving intermediates
        :param intermediate_save_dir: a directory for saving feature pickle files
        :param number_of_smali_files: the maximum number of smali files processed
        :param max_vocab_size: the maximum number of words
        :param file_ext: file extension
        :param update: boolean indicator for recomputing the naive features
        :param proc_number: process number
        """
        self.naive_data_save_dir = naive_data_save_dir
        self.intermediate_save_dir = intermediate_save_dir
        self.maximum_vocab_size = max_vocab_size
        self.number_of_smali_files = number_of_smali_files

        self.file_ext = file_ext
        self.update = update
        self.proc_number = proc_number

        if len(kwargs) > 0:
            logger.warning("unused hyper parameters {}.".format(kwargs))

    # è¿™æ®µä»£ç å®šä¹‰äº† Apk2features ç±»çš„ feature_extraction æ–¹æ³•ï¼Œ
    # ç”¨äºä»æŒ‡å®šç›®å½•ä¸­çš„ APK æ–‡ä»¶ä¸­æå–ç‰¹å¾å¹¶ä¿å­˜ã€‚æ–¹æ³•è¿”å›æå–ç‰¹å¾åçš„æ–‡ä»¶è·¯å¾„ã€‚
    def feature_extraction(self, sample_dir):
        """ save the android features and return the saved paths """
        sample_path_list = utils.check_dir(sample_dir)
        pool = multiprocessing.Pool(self.proc_number, initializer=utils.pool_initializer)

        # å®šä¹‰ä¸€ä¸ªåä¸º get_save_path çš„å†…éƒ¨å‡½æ•°ï¼Œç”¨äºè·å–ç‰¹å¾ä¿å­˜è·¯å¾„ã€‚
        # å®ƒæ ¹æ® APK æ–‡ä»¶çš„ SHA256 ç¼–ç å’Œæ–‡ä»¶æ‰©å±•åç”Ÿæˆä¿å­˜è·¯å¾„ã€‚
        # å¦‚æœè¯¥è·¯å¾„å¯¹åº”çš„æ–‡ä»¶å·²å­˜åœ¨ï¼Œå¹¶ä¸”ä¸éœ€è¦æ›´æ–°ç‰¹å¾ï¼Œåˆ™è¿”å› Noneã€‚å¦åˆ™ï¼Œè¿”å›ä¿å­˜è·¯å¾„ã€‚
        def get_save_path(a_path):
            sha256_code = os.path.splitext(os.path.basename(a_path))[0]  # utils.get_sha256(apk_path)
            save_path = os.path.join(self.naive_data_save_dir, sha256_code + self.file_ext)

            if os.path.exists(save_path) and (not self.update):
                return
            else:
                return save_path
            
        # åˆ›å»ºä¸€ä¸ªåä¸º params çš„åˆ—è¡¨ï¼ŒåŒ…å«éœ€è¦æå–ç‰¹å¾çš„ APK æ–‡ä»¶è·¯å¾„ã€å¤„ç†çš„ smali æ–‡ä»¶æœ€å¤§æ•°é‡å’Œç‰¹å¾ä¿å­˜è·¯å¾„ã€‚
        # åªæœ‰å½“ get_save_path è¿”å›å€¼ä¸ä¸º None æ—¶ï¼Œæ‰å°† APK æ–‡ä»¶è·¯å¾„æ·»åŠ åˆ° params åˆ—è¡¨ä¸­ã€‚
        params = [(apk_path, self.number_of_smali_files, get_save_path(apk_path)) for \
                  apk_path in sample_path_list if get_save_path(apk_path) is not None]
        
        # ä½¿ç”¨ pool.imap_unordered() æ–¹æ³•å¹¶è¡Œåœ°å¯¹ params ä¸­çš„æ¯ä¸ªå…ƒç´ æ‰§è¡Œ feature_gen.apk2feat_wrapper å‡½æ•°ã€‚
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºå¤„ç†è¿›åº¦ã€‚å¦‚æœå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸ï¼Œä½¿ç”¨ logger.error è¾“å‡ºé”™è¯¯ä¿¡æ¯ã€‚
        for res in tqdm(pool.imap_unordered(feature_gen.apk2feat_wrapper, params), total=len(params)):
            if isinstance(res, Exception):
                logger.error("Failed processing: {}".format(str(res)))
        pool.close()
        pool.join()

        feature_paths = []
        
        # éå† sample_path_listï¼Œè·å–æ¯ä¸ª APK æ–‡ä»¶çš„ç‰¹å¾ä¿å­˜è·¯å¾„ã€‚
        # å¦‚æœè·¯å¾„å¯¹åº”çš„æ–‡ä»¶å­˜åœ¨ï¼Œåˆ™å°†å…¶æ·»åŠ åˆ° feature_paths åˆ—è¡¨ä¸­ã€‚
        for i, apk_path in enumerate(sample_path_list):
            sha256_code = os.path.splitext(os.path.basename(apk_path))[0]  # utils.get_sha256(apk_path)
            save_path = os.path.join(self.naive_data_save_dir, sha256_code + self.file_ext)
            if os.path.exists(save_path):
                feature_paths.append(save_path)

        return feature_paths


    def get_vocab(self, feature_path_list=None, gt_labels=None):
        """
        get vocabularies incorporating feature selection
        :param feature_path_list: feature_path_list, list, a list of paths, 
        each of which directs to a feature file (we \
        suggest using the feature files for the training purpose)
        :param gt_labels: gt_labels, list or numpy.ndarray, ground truth labels
        :return: list, a list of words
        
        feature_path_listï¼šç‰¹å¾æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œæ¯ä¸ªè·¯å¾„æŒ‡å‘ä¸€ä¸ªç‰¹å¾æ–‡ä»¶ã€‚
        gt_labelsï¼šçœŸå®æ ‡ç­¾ï¼Œè¡¨ç¤ºæ¯ä¸ªç‰¹å¾æ–‡ä»¶å¯¹åº”çš„æ¶æ„è½¯ä»¶æˆ–è‰¯æ€§æ ·æœ¬ã€‚
        æ–¹æ³•è¿”å›ä¸€ä¸ªåŒ…å«è¯æ±‡è¡¨ã€è¯æ±‡ä¿¡æ¯å’Œè¯æ±‡ç±»å‹çš„å…ƒç»„ã€‚
        """
        vocab_saving_path = os.path.join(self.intermediate_save_dir, 'data.vocab')
        vocab_type_saving_path = os.path.join(self.intermediate_save_dir, 'data.vocab_type')
        vocab_extra_info_saving_path = os.path.join(self.intermediate_save_dir, 'data.vocab_info')
        
        # å¦‚æœè¿™äº›æ–‡ä»¶å·²ç»å­˜åœ¨ä¸”ä¸éœ€è¦æ›´æ–°ï¼Œä»æ–‡ä»¶ä¸­è¯»å–å¹¶è¿”å›è¯æ±‡è¡¨ã€è¯æ±‡ä¿¡æ¯å’Œè¯æ±‡ç±»å‹ã€‚
        if os.path.exists(vocab_saving_path) and os.path.exists(vocab_saving_path) and (not self.update):
            return utils.read_pickle(vocab_saving_path), utils.read_pickle(vocab_extra_info_saving_path), utils.read_pickle(vocab_type_saving_path)
        elif feature_path_list is None and gt_labels is None:
            raise FileNotFoundError("No vocabulary found and no features for producing vocabulary!")
        else:
            pass
        
        # ç¡®ä¿è¾“å…¥çš„æ¶æ„è½¯ä»¶å’Œè‰¯æ€§æ ·æœ¬æ ‡ç­¾éƒ½å­˜åœ¨ï¼Œå¹¶æ£€æŸ¥
        # feature_path_list å’Œ gt_labels çš„é•¿åº¦æ˜¯å¦ç›¸ç­‰ã€‚
        assert not (np.all(gt_labels == 1) or np.all(gt_labels == 0)), 'Expect both malware and benign samples.'
        assert len(feature_path_list) == len(gt_labels)

        # ä½¿ç”¨ collections.Counter å’Œ collections.defaultdict åˆ›å»ºè®¡æ•°å™¨å’Œå­—å…¸ä»¥å­˜å‚¨è¯æ±‡è¡¨ç›¸å…³ä¿¡æ¯ã€‚
        counter_mal, counter_ben = collections.Counter(), collections.Counter()
        feat_info_dict = collections.defaultdict(set)
        feat_type_dict = collections.defaultdict(str)
        
        # éå† feature_path_list å’Œ gt_labels
        for feature_path, label in zip(feature_path_list, gt_labels):
            if not os.path.exists(feature_path):
                continue
            features = feature_gen.read_from_disk(feature_path)
            # è·å–ç‰¹å¾åˆ—è¡¨ã€ç‰¹å¾ä¿¡æ¯åˆ—è¡¨å’Œç‰¹å¾ç±»å‹åˆ—è¡¨ã€‚
            # æ ¹æ®æ ‡ç­¾æ›´æ–°æ¶æ„è½¯ä»¶å’Œè‰¯æ€§æ ·æœ¬çš„è®¡æ•°å™¨ã€‚
            feature_list, feature_info_list, feature_type_list = feature_gen.get_feature_list(features)
            feature_occurrence = list(dict.fromkeys(feature_list))
            for _feat, _feat_info, _feat_type in zip(feature_list, feature_info_list, feature_type_list):
                feat_info_dict[_feat].add(_feat_info)
                feat_type_dict[_feat] = _feat_type
            if label:
                counter_mal.update(list(feature_occurrence))
            else:
                counter_ben.update(list(feature_occurrence))
        all_words = list(dict.fromkeys(list(counter_ben.keys()) + list(counter_mal.keys())))
        if len(all_words) <= 0:
            raise ValueError("No features exist on this dataset.")

        # æ ¹æ®ç‰¹å¾é€‰æ‹©ç­–ç•¥é€‰æ‹©è¯æ±‡
        maximum_vocab_size = self.maximum_vocab_size
        selected_words = []
        
        # ----------------------------------------
        # dangerous permission
        # å±é™©æƒé™é€‰æ‹©ï¼šæå–è¯æ±‡è¡¨ä¸­çš„å±é™©æƒé™ç‰¹å¾ï¼Œå¹¶å¯¹æ¯ä¸ªæƒé™è¿›è¡Œæ£€æŸ¥ã€‚
        # å¦‚æœæƒé™è¢«è®¤ä¸ºæ˜¯å±é™©çš„ï¼ˆé€šè¿‡ feature_gen.permission_check å‡½æ•°åˆ¤æ–­ï¼‰ï¼Œ
        # åˆ™å°†å…¶æ·»åŠ åˆ° selected_words åˆ—è¡¨ä¸­ã€‚
        all_words_type = list(map(feat_type_dict.get, all_words))
        perm_pos = np.array(all_words_type)[...] == feature_gen.PERMISSION
        perm_features = np.array(all_words)[perm_pos]
        for perm in perm_features:
            if feature_gen.permission_check(perm):
                selected_words.append(perm)

        # intent
        # æ„å›¾é€‰æ‹©ï¼šæå–è¯æ±‡è¡¨ä¸­çš„æ„å›¾ç‰¹å¾ï¼Œå¹¶å¯¹æ¯ä¸ªæ„å›¾è¿›è¡Œæ£€æŸ¥ã€‚
        # å¦‚æœæ„å›¾è¢«è®¤ä¸ºæ˜¯æœ‰å®³çš„ï¼ˆé€šè¿‡ feature_gen.intent_action_check å‡½æ•°åˆ¤æ–­ï¼‰ï¼Œ
        # åˆ™å°†å…¶æ·»åŠ åˆ° selected_words åˆ—è¡¨ä¸­ã€‚
        intent_pos = np.array(all_words_type)[...] == feature_gen.INTENT
        intent_features = np.array(all_words)[intent_pos]
        for intent in intent_features:
            if feature_gen.intent_action_check(intent):
                selected_words.append(intent)

        # suspicious apis
        # å¯ç–‘ API é€‰æ‹©ï¼šæå–è¯æ±‡è¡¨ä¸­çš„ç³»ç»Ÿ API ç‰¹å¾ï¼Œå¹¶å¯¹æ¯ä¸ª API è¿›è¡Œæ£€æŸ¥ã€‚
        # å¦‚æœ API è¢«è®¤ä¸ºæ˜¯å¯ç–‘çš„æˆ–æ•æ„Ÿçš„ï¼ˆé€šè¿‡ feature_gen.check_suspicious_api æˆ– feature_gen.check_sensitive_api å‡½æ•°åˆ¤æ–­ï¼‰ï¼Œ
        # åˆ™å°†å…¶æ·»åŠ åˆ° selected_words åˆ—è¡¨ä¸­ã€‚
        api_pos = np.array(all_words_type)[...] == feature_gen.SYS_API
        susp_apis = np.array(all_words)[api_pos]
        for api in susp_apis:
            if feature_gen.check_suspicious_api(api) or feature_gen.check_sensitive_api(api):
                selected_words.append(api)
        # ----------------------------------------
        
        # remove components
        # ç§»é™¤ç»„ä»¶ï¼šä»è¯æ±‡è¡¨ä¸­ç§»é™¤æ‰€æœ‰å±äºæ´»åŠ¨ã€æœåŠ¡ã€æ¥æ”¶å™¨å’Œæä¾›å™¨çš„ç»„ä»¶ã€‚
        api_comps = np.array(all_words_type)[...] == feature_gen.ACTIVITY
        api_comps = api_comps | (np.array(all_words_type)[...] == feature_gen.SERVICE)
        api_comps = api_comps | (np.array(all_words_type)[...] == feature_gen.RECEIVER)
        api_comps = api_comps | (np.array(all_words_type)[...] == feature_gen.PROVIDER)
                
        # è®¡ç®—æ¶æ„è½¯ä»¶å’Œè‰¯æ€§æ ·æœ¬çš„ç‰¹å¾é¢‘ç‡å·®å¹¶æ ¹æ®å·®å¼‚å¯¹è¯æ±‡è¿›è¡Œæ’åºã€‚
        # é€‰æ‹©æœ€å¤š maximum_vocab_size ä¸ªè¯æ±‡ã€‚
        all_words = list(np.array(all_words)[~api_comps])
        for s_word in selected_words:
            all_words.remove(s_word)
        logger.info("The total number of words: {}-{}.".format(len(selected_words), len(all_words)))

        # è®¡ç®—æ¶æ„æ ·æœ¬çš„ç‰¹å¾é¢‘ç‡
        mal_feature_frequency = np.array(list(map(counter_mal.get, all_words)))
        mal_feature_frequency[mal_feature_frequency == None] = 0
        mal_feature_frequency = mal_feature_frequency.astype(np.float64)
        mal_feature_frequency /= np.sum(gt_labels)

        # è®¡ç®—è‰¯æ€§æ ·æœ¬çš„ç‰¹å¾é¢‘ç‡
        ben_feature_frequency = np.array(list(map(counter_ben.get, all_words)))
        ben_feature_frequency[ben_feature_frequency == None] = 0
        ben_feature_frequency = ben_feature_frequency.astype(np.float64)
        ben_feature_frequency /= float(len(gt_labels) - np.sum(gt_labels))

        # è®¡ç®—ç‰¹å¾é¢‘ç‡å·®
        feature_freq_diff = abs(mal_feature_frequency - ben_feature_frequency)

        # æ ¹æ®ç‰¹å¾é¢‘ç‡å·®è¿›è¡Œæ’åº
        posi_selected = np.argsort(feature_freq_diff)[::-1]
        ordered_words = selected_words + [all_words[p] for p in posi_selected]

        # é€‰æ‹©æœ€å¤š maximum_vocab_size ä¸ªè¯æ±‡
        selected_words = ordered_words[:maximum_vocab_size]

        # è·å–æ‰€é€‰è¯æ±‡çš„ç±»å‹å’Œå¯¹åº”çš„è¯æ±‡ä¿¡æ¯ï¼š
        # ä½¿ç”¨ feat_type_dict å’Œ feat_info_dict å­—å…¸åˆ†åˆ«è·å–æ‰€é€‰è¯æ±‡çš„ç±»å‹å’Œå¯¹åº”çš„è¯æ±‡ä¿¡æ¯ï¼Œä»¥ä¾¿åœ¨ä¹‹åçš„å¤„ç†ä¸­ä½¿ç”¨ã€‚
        selected_word_type = list(map(feat_type_dict.get, selected_words))
        corresponding_word_info = list(map(feat_info_dict.get, selected_words))

        # ä¿å­˜æ‰€é€‰è¯æ±‡ã€è¯æ±‡ç±»å‹å’Œå¯¹åº”è¯æ±‡ä¿¡æ¯åˆ°æ–‡ä»¶ï¼Œç„¶åè¿”å›è¿™äº›å€¼
        if len(selected_words) > 0:
            utils.dump_pickle(selected_words, vocab_saving_path)
            utils.dump_pickle(selected_word_type, vocab_type_saving_path)
            utils.dump_pickle(corresponding_word_info, vocab_extra_info_saving_path)
        return selected_words, corresponding_word_info, selected_word_type
    
        
    def feature_mapping(self, feature_path_list, dictionary):
        """
        mapping feature to numerical representation
        :param feature_path_list: a list of feature paths
        :param dictionary: vocabulary -> index
        :return: 2D representation
        :rtype numpy.ndarray
        """
        raise NotImplementedError

    @staticmethod
    def get_non_api_size(vocabulary=None):
        cursor = 0
        for word in vocabulary:
            if '->' not in word:  # exclude the api features
                cursor += 1
            else:
                break
        return cursor

    def get_cached_name(self, feature_path):
        if os.path.isfile(feature_path):
            return os.path.splitext(os.path.basename(feature_path))[0] + '.npz'
        else:
            raise FileNotFoundError

    # â­ è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º feature2ipt çš„æ–¹æ³•ï¼Œå®ƒå°†åº”ç”¨ç¨‹åºçš„ç‰¹å¾æ˜ å°„åˆ°æ•°å€¼è¡¨ç¤ºã€‚
    # feature2ipt æ–¹æ³•çš„ä¸»è¦ç›®çš„æ˜¯å°†åº”ç”¨ç¨‹åºçš„ç‰¹å¾æ˜ å°„åˆ°ä¸€ä¸ªå›ºå®šé•¿åº¦çš„å‘é‡ï¼Œ
    # å…¶ä¸­æ¯ä¸ªå…ƒç´ è¡¨ç¤ºå¯¹åº”è¯æ±‡è¡¨ä¸­å•è¯çš„å­˜åœ¨ï¼ˆ1ï¼‰æˆ–ä¸å­˜åœ¨ï¼ˆ0ï¼‰ã€‚
    # è¿™æ ·çš„æ•°å€¼è¡¨ç¤ºå¯ä»¥ä½œä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¾“å…¥ï¼Œä»¥ä¾¿å¯¹åº”ç”¨ç¨‹åºè¿›è¡Œåˆ†ç±»æˆ–å…¶ä»–åˆ†æä»»åŠ¡ã€‚
    def feature2ipt(self, feature_path, label, vocabulary=None, cache_dir=None):
        """
        Map features to numerical representations

        Parameters
        --------
        :param feature_path, string, a path directs to a feature file
        :param label, int, ground truth labels
        :param vocabulary:list, a list of words
        :param cache_dir: a temporal folder
        :return: numerical representations corresponds to an app. Each representation contains a tuple
        (feature 1D array, label)
        """
        # ç¡®ä¿è¯æ±‡è¡¨ä¸ä¸ºç©º
        assert vocabulary is not None and len(vocabulary) > 0
        
        # æ£€æŸ¥ç¼“å­˜ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™åŠ è½½ç¼“å­˜æ•°æ®
        if isinstance(cache_dir, str):
            rpst_cached_name = self.get_cached_name(feature_path)
            rpst_cached_path = os.path.join(cache_dir, rpst_cached_name)
            if os.path.exists(rpst_cached_path):
                return utils.read_pickle(rpst_cached_path, use_gzip=True)
            
        # å¦‚æœ feature_path æ— æ•ˆï¼Œåˆ™è¿”å›é›¶å‘é‡è¡¨ç¤º
        if not isinstance(feature_path, str):
            logger.warning("Cannot find the feature path: {}, zero vector used".format(feature_path))
            return np.zeros((len(vocabulary), ), dtype=np.float32), []

        if not os.path.exists(feature_path):
            logger.warning("Cannot find the feature path: {}, zero vector used".format(feature_path))
            return np.zeros((len(vocabulary), ), dtype=np.float32), []

        # ä»ç»™å®šçš„ feature_path åŠ è½½åŸå§‹ç‰¹å¾ï¼Œå¹¶å°†å…¶æ ¼å¼åŒ–ä¸ºé API ç‰¹å¾å’Œ API ç‰¹å¾ã€‚
        native_features = feature_gen.read_from_disk(feature_path)
        non_api_features, api_features = feature_gen.format_feature(native_features)
        features = non_api_features + api_features

        # åˆå§‹åŒ–ä¸€ä¸ªé•¿åº¦ä¸è¯æ±‡è¡¨ç›¸ç­‰çš„é›¶å‘é‡ï¼ˆrepresentation_vectorï¼‰ä½œä¸ºæ•°å€¼è¡¨ç¤ºã€‚
        representation_vector = np.zeros((len(vocabulary), ), dtype=np.float32)
        
        # å°†è¯æ±‡è¡¨æ˜ å°„åˆ°å…¶ç´¢å¼•ï¼Œå¹¶æ ¹æ®æå–åˆ°çš„ç‰¹å¾å¡«å…… representation_vectorã€‚
        dictionary = dict(zip(vocabulary, range(len(vocabulary))))
        filled_pos = [idx for idx in list(map(dictionary.get, features)) if idx is not None]
        
        if len(filled_pos) > 0:
            representation_vector[filled_pos] = 1.
        return representation_vector, label

def _main():
    # project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    # sys.path.insert(0, project_root)
    # sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..')))
    
    from config import config
    
    malware_dir_name = config.get('dataset', 'malware_dir')
    benign_dir_name = config.get('dataset', 'benware_dir')
    meta_data_saving_dir = config.get('dataset', 'intermediate')
    naive_data_saving_dir = config.get('metadata', 'naive_data_pool')
    
    feature_extractor = Apk2features(naive_data_saving_dir,
                                     meta_data_saving_dir,
                                     update=False,
                                     proc_number=2)
    
    mal_paths = feature_extractor.feature_extraction(malware_dir_name)
    pprint(mal_paths)
    
    ben_paths = feature_extractor.feature_extraction(benign_dir_name)
    pprint(ben_paths)
    
    labels = np.zeros((len(mal_paths) + len(ben_paths), ))
    labels[:len(mal_paths)] = 1
    pprint(labels)
    
    # è·å–è¯æ±‡è¡¨ vocab
    # ğŸ– å‚æ•°å¯¹ä¸ä¸Š
    # vocab, _1 = feature_extractor.get_vocab(mal_paths + ben_paths, labels)
    vocab, vocab1, vocab2 = feature_extractor.get_vocab(mal_paths + ben_paths, labels)
    # pprint(vocab)
    # pprint(vocab1)
    # pprint(vocab2)
    
    # ä½¿ç”¨ feature_extractor.feature2ipt() æ–¹æ³•å°†æ¶æ„è½¯ä»¶ç›®å½•ä¸­çš„ç¬¬ä¸€ä¸ª APK æ–‡ä»¶çš„ç‰¹å¾è½¬æ¢ä¸ºè¾“å…¥è¡¨ç¤ºï¼Œ
    # åŒæ—¶ä¼ å…¥è¯æ±‡è¡¨ vocabã€‚ç»“æœå­˜å‚¨åœ¨ n_rpst å’Œ api_rpst ä¸­ã€‚
    # ğŸ– å‚æ•°å¯¹ä¸ä¸Š
    # n_rpst, api_rpst, _1 = feature_extractor.feature2ipt(mal_paths[0], label=1, vocabulary=vocab)
    
    for i in range(len(mal_paths)):
        n_rpst, api_rpst = feature_extractor.feature2ipt(mal_paths[i], label=1, vocabulary=vocab)
        print(n_rpst)
        print(n_rpst.shape)
        print(api_rpst)
    
    # print(api_rpst)

if __name__ == "__main__":
    import sys

    sys.exit(_main())